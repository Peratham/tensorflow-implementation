{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import ipdb\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cPickle\n",
    "\n",
    "from tensorflow.models.rnn import rnn_cell\n",
    "import tensorflow.python.platform\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Caption_Generator():\n",
    "\n",
    "    def init_weight(self, dim_in, dim_out, name=None, stddev=1.0):\n",
    "        return tf.Variable(tf.truncated_normal([dim_in, dim_out], stddev=stddev/math.sqrt(float(dim_in))), name=name)\n",
    "\n",
    "    def init_bias(self, dim_out, name=None):\n",
    "        return tf.Variable(tf.zeros([dim_out]), name=name)\n",
    "\n",
    "    def __init__(self, n_words, dim_embed, dim_ctx, dim_hidden, n_lstm_steps, batch_size=200, ctx_shape=[196,512], bias_init_vector=None):\n",
    "        self.n_words = n_words\n",
    "        self.dim_embed = dim_embed\n",
    "        self.dim_ctx = dim_ctx\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.ctx_shape = ctx_shape\n",
    "        self.n_lstm_steps = n_lstm_steps\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            self.Wemb = tf.Variable(tf.random_uniform([n_words, dim_embed], -1.0, 1.0), name='Wemb')\n",
    "\n",
    "        self.init_hidden_W = self.init_weight(dim_ctx, dim_hidden, name='init_hidden_W')\n",
    "        self.init_hidden_b = self.init_bias(dim_hidden, name='init_hidden_b')\n",
    "\n",
    "        self.init_memory_W = self.init_weight(dim_ctx, dim_hidden, name='init_memory_W')\n",
    "        self.init_memory_b = self.init_bias(dim_hidden, name='init_memory_b')\n",
    "\n",
    "        self.lstm_W = self.init_weight(dim_embed, dim_hidden*4, name='lstm_W')\n",
    "        self.lstm_U = self.init_weight(dim_hidden, dim_hidden*4, name='lstm_U')\n",
    "        self.lstm_b = self.init_bias(dim_hidden*4, name='lstm_b')\n",
    "\n",
    "        self.image_encode_W = self.init_weight(dim_ctx, dim_hidden*4, name='image_encode_W')\n",
    "\n",
    "        self.image_att_W = self.init_weight(dim_ctx, dim_ctx, name='image_att_W')\n",
    "        self.hidden_att_W = self.init_weight(dim_hidden, dim_ctx, name='hidden_att_W')\n",
    "        self.pre_att_b = self.init_bias(dim_ctx, name='pre_att_b')\n",
    "\n",
    "        self.att_W = self.init_weight(dim_ctx, 1, name='att_W')\n",
    "        self.att_b = self.init_bias(1, name='att_b')\n",
    "\n",
    "        self.decode_lstm_W = self.init_weight(dim_hidden, dim_embed, name='decode_lstm_W')\n",
    "        self.decode_lstm_b = self.init_bias(dim_embed, name='decode_lstm_b')\n",
    "\n",
    "        self.decode_word_W = self.init_weight(dim_embed, n_words, name='decode_word_W')\n",
    "\n",
    "        if bias_init_vector is not None:\n",
    "            self.decode_word_b = tf.Variable(bias_init_vector.astype(np.float32), name='decode_word_b')\n",
    "        else:\n",
    "            self.decode_word_b = self.init_bias(n_words, name='decode_word_b')\n",
    "\n",
    "\n",
    "    def get_initial_lstm(self, mean_context):\n",
    "        initial_hidden = tf.nn.tanh(tf.matmul(mean_context, self.init_hidden_W) + self.init_hidden_b)\n",
    "        initial_memory = tf.nn.tanh(tf.matmul(mean_context, self.init_memory_W) + self.init_memory_b)\n",
    "\n",
    "        return initial_hidden, initial_memory\n",
    "\n",
    "    def build_model(self):\n",
    "        context = tf.placeholder(\"float32\", [self.batch_size, self.ctx_shape[0], self.ctx_shape[1]])\n",
    "        sentence = tf.placeholder(\"int32\", [self.batch_size, self.n_lstm_steps])\n",
    "        mask = tf.placeholder(\"float32\", [self.batch_size, self.n_lstm_steps])\n",
    "\n",
    "        h, c = self.get_initial_lstm(tf.reduce_mean(context, 1))\n",
    "\n",
    "        context_flat = tf.reshape(context, [-1, self.dim_ctx])\n",
    "        context_encode = tf.matmul(context_flat, self.image_att_W) # (batch_size, 196, 512)\n",
    "        context_encode = tf.reshape(context_encode, [-1, ctx_shape[0], ctx_shape[1]])\n",
    "\n",
    "        loss = 0.0\n",
    "\n",
    "        for ind in range(self.n_lstm_steps):\n",
    "\n",
    "            if ind == 0:\n",
    "                word_emb = tf.zeros([self.batch_size, self.dim_embed])\n",
    "            else:\n",
    "                tf.get_variable_scope().reuse_variables()\n",
    "                with tf.device(\"/cpu:0\"):\n",
    "                    word_emb = tf.nn.embedding_lookup(self.Wemb, sentence[:,ind-1])\n",
    "\n",
    "            x_t = tf.matmul(word_emb, self.lstm_W) + self.lstm_b # (batch_size, hidden*4)\n",
    "\n",
    "            labels = tf.expand_dims(sentence[:,ind], 1)\n",
    "            indices = tf.expand_dims(tf.range(0, self.batch_size, 1), 1)\n",
    "            concated = tf.concat(1, [indices, labels])\n",
    "            onehot_labels = tf.sparse_to_dense( concated, tf.pack([self.batch_size, self.n_words]), 1.0, 0.0)\n",
    "\n",
    "            context_encode = context_encode + \\\n",
    "                 tf.expand_dims(tf.matmul(h, self.hidden_att_W), 1) + \\\n",
    "                 self.pre_att_b\n",
    "\n",
    "            context_encode = tf.nn.tanh(context_encode)\n",
    "\n",
    "            # 여기도 context_encode: 3D -> flat required\n",
    "            context_encode_flat = tf.reshape(context_encode, [-1, self.dim_ctx]) # (batch_size*196, 512)\n",
    "            alpha = tf.matmul(context_encode_flat, self.att_W) + self.att_b # (batch_size*196, 1)\n",
    "            alpha = tf.reshape(alpha, [-1, self.ctx_shape[0]])\n",
    "            alpha = tf.nn.softmax( alpha )\n",
    "\n",
    "            weighted_context = tf.reduce_sum(context * tf.expand_dims(alpha, 2), 1)\n",
    "\n",
    "            lstm_preactive = tf.matmul(h, self.lstm_U) + x_t + tf.matmul(weighted_context, self.image_encode_W)\n",
    "            i, f, o, new_c = tf.split(1, 4, lstm_preactive)\n",
    "\n",
    "            i = tf.nn.sigmoid(i)\n",
    "            f = tf.nn.sigmoid(f)\n",
    "            o = tf.nn.sigmoid(o)\n",
    "            new_c = tf.nn.tanh(new_c)\n",
    "\n",
    "            c = f * c + i * new_c\n",
    "            h = o * tf.nn.tanh(new_c)\n",
    "\n",
    "            logits = tf.matmul(h, self.decode_lstm_W) + self.decode_lstm_b\n",
    "            logits = tf.nn.relu(logits)\n",
    "            logits = tf.nn.dropout(logits, 0.5)\n",
    "\n",
    "            logit_words = tf.matmul(logits, self.decode_word_W) + self.decode_word_b\n",
    "            cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logit_words, onehot_labels)\n",
    "            cross_entropy = cross_entropy * mask[:,ind]\n",
    "\n",
    "            current_loss = tf.reduce_sum(cross_entropy)\n",
    "            loss = loss + current_loss\n",
    "\n",
    "        loss = loss / tf.reduce_sum(mask)\n",
    "        return loss, context, sentence, mask\n",
    "\n",
    "    def build_generator(self, maxlen):\n",
    "        context = tf.placeholder(\"float32\", [1, self.ctx_shape[0], self.ctx_shape[1]])\n",
    "        h, c = self.get_initial_lstm(tf.reduce_mean(context, 1))\n",
    "\n",
    "        context_encode = tf.matmul(tf.squeeze(context), self.image_att_W)\n",
    "        generated_words = []\n",
    "        logit_list = []\n",
    "        alpha_list = []\n",
    "        word_emb = tf.zeros([1, self.dim_embed])\n",
    "        for ind in range(maxlen):\n",
    "            x_t = tf.matmul(word_emb, self.lstm_W) + self.lstm_b\n",
    "            context_encode = context_encode + tf.matmul(h, self.hidden_att_W) + self.pre_att_b\n",
    "            context_encode = tf.nn.tanh(context_encode)\n",
    "\n",
    "            alpha = tf.matmul(context_encode, self.att_W) + self.att_b\n",
    "            alpha = tf.reshape(alpha, [-1, self.ctx_shape[0]] )\n",
    "            alpha = tf.nn.softmax(alpha)\n",
    "\n",
    "            alpha = tf.reshape(alpha, (ctx_shape[0], -1))\n",
    "            alpha_list.append(alpha)\n",
    "\n",
    "            weighted_context = tf.reduce_sum(tf.squeeze(context) * alpha, 0)\n",
    "            weighted_context = tf.expand_dims(weighted_context, 0)\n",
    "\n",
    "            lstm_preactive = tf.matmul(h, self.lstm_U) + x_t + tf.matmul(weighted_context, self.image_encode_W)\n",
    "\n",
    "            i, f, o, new_c = tf.split(1, 4, lstm_preactive)\n",
    "\n",
    "            i = tf.nn.sigmoid(i)\n",
    "            f = tf.nn.sigmoid(f)\n",
    "            o = tf.nn.sigmoid(o)\n",
    "            new_c = tf.nn.tanh(new_c)\n",
    "\n",
    "            c = f*c + i*new_c\n",
    "            h = o*tf.nn.tanh(new_c)\n",
    "\n",
    "            logits = tf.matmul(h, self.decode_lstm_W) + self.decode_lstm_b\n",
    "            logits = tf.nn.relu(logits)\n",
    "\n",
    "            logit_words = tf.matmul(logits, self.decode_word_W) + self.decode_word_b\n",
    "\n",
    "            max_prob_word = tf.argmax(logit_words, 1)\n",
    "\n",
    "            with tf.device(\"/cpu:0\"):\n",
    "                word_emb = tf.nn.embedding_lookup(self.Wemb, max_prob_word)\n",
    "\n",
    "            generated_words.append(max_prob_word)\n",
    "            logit_list.append(logit_words)\n",
    "\n",
    "        return context, generated_words, logit_list, alpha_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
