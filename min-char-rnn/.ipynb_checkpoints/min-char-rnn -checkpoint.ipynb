{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# min-char-rnn 링크: https://gist.github.com/karpathy/d4dee566867f8291f086\n",
    "#\n",
    "# output: ['e']  ['l']  ['l']  ['o']  ['w']    min-char-rnn tensorflow 버전 구현.\n",
    "#           ^      ^      ^      ^      ^      input.txt에 있는 문장을 language model을 이용하여 생성하는 것이 목표. \n",
    "#           |      |      |      |      |      \n",
    "#          +-+    +-+    +-+    +-+    +-+     \n",
    "#          |1| -> |2| -> |3| -> |4| -> |5|     \n",
    "#          +-+    +-+    +-+    +-+    +-+     input에는 마지막 문자('w')를 제외시키고\n",
    "#           ^      ^      ^      ^      ^      output에는 첫 문자('h')를 제외시키므로\n",
    "#           |      |      |      |      |      time_step_size = (전체 문장의 길이 - 1)가 된다.\n",
    "# input : ['h']  ['e']  ['l']  ['l']  ['o']\n",
    "\n",
    "\n",
    "\n",
    "# 참고: tf.nn.rnn_cell과 tf.nn.rnn 함수를 사용하는 방법을 이해하는 데에 오래 걸렸다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 66 characters, 23 unique.\n"
     ]
    }
   ],
   "source": [
    "# 학습할 데이터를 불러온다\n",
    "data = open('input.txt', 'r').read() \n",
    "start_char = data[0]\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "\n",
    "print 'data has %d characters, %d unique.' % (data_size, vocab_size)\n",
    "char_to_idx = {ch:idx for idx,ch in enumerate(chars)}\n",
    "idx_to_char = {idx:ch for idx,ch in enumerate(chars)}\n",
    "\n",
    "# hyperparameters\n",
    "num_epochs = 301\n",
    "num_sampling = 10\n",
    "hidden_size = 100 \n",
    "num_time_steps = data_size -1\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# model parameters\n",
    "W = tf.Variable(tf.truncated_normal([hidden_size, vocab_size], stddev=0.1)) \n",
    "b = tf.Variable(tf.zeros([vocab_size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input data의 차원:  (65, 23)\n",
      "out data의 차원:  (65, 23)\n",
      "list의 길이:  65\n",
      "각 원소의 차원:  (1, 23)\n"
     ]
    }
   ],
   "source": [
    "# generate character index dataset\n",
    "X = []\n",
    "y = []\n",
    "for i in range(data_size - 1):\n",
    "    X.append(char_to_idx[data[i]])\n",
    "    y.append(char_to_idx[data[i+1]])\n",
    "    \n",
    "X_onehot = tf.cast(tf.one_hot(X, vocab_size, on_value=1), tf.float32)\n",
    "y_onehot = tf.cast(tf.one_hot(y, vocab_size, on_value=1), tf.float32)\n",
    "print \"input data의 차원: \", X_onehot.get_shape()\n",
    "print \"out data의 차원: \", y_onehot.get_shape()\n",
    "\n",
    "# tf.nn.rnn 함수에 인자로 넣어주기 위해 형식을 맞춰준다 time_step @ (batch_size, input_size)\n",
    "X_split = tf.split(0, num_time_steps, X_onehot)\n",
    "print \"list의 길이: \", len(X_split) # 60 @ (1, 19)\n",
    "print \"각 원소의 차원: \", X_split[0].get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def model():\n",
    "    cell = tf.nn.rnn_cell.BasicRNNCell(hidden_size)\n",
    "    outputs, _ = tf.nn.rnn(cell, X_split, dtype=tf.float32) # 60 @ (1, 100)\n",
    "    h = tf.concat(0, outputs) # (60, 100)\n",
    "    logits = tf.matmul(h, W) + b  # (60, 19)\n",
    "    loss = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(logits, y_onehot))\n",
    "    max_char = tf.argmax(logits, 1)\n",
    "    return loss, max_char\n",
    "\n",
    "loss, max_char = model()\n",
    "optimizer = tf.train.RMSPropOptimizer(0.01).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at epoch 0 :  205.755\n",
      "생성된 문장: HiNN oieepsplsaReeeeeaeeueeNsHpeeleplesneninpheepauaaiepeeeeeosinn\n",
      "loss at epoch 10 :  169.027\n",
      "loss at epoch 20 :  159.723\n",
      "loss at epoch 30 :  128.92\n",
      "loss at epoch 40 :  91.6823\n",
      "loss at epoch 50 :  13.2543\n",
      "loss at epoch 60 :  4.14548\n",
      "loss at epoch 70 :  1.68452\n",
      "loss at epoch 80 :  0.807435\n",
      "loss at epoch 90 :  0.414715\n",
      "loss at epoch 100 :  0.221801\n",
      "생성된 문장: Hello, this is a sample sentence for training RNN language model. \n",
      "loss at epoch 110 :  0.121619\n",
      "loss at epoch 120 :  0.0677693\n",
      "loss at epoch 130 :  0.0381707\n",
      "loss at epoch 140 :  0.0216569\n",
      "loss at epoch 150 :  0.0123494\n",
      "loss at epoch 160 :  0.00706621\n",
      "loss at epoch 170 :  0.00405509\n",
      "loss at epoch 180 :  0.0023318\n",
      "loss at epoch 190 :  0.00134442\n",
      "loss at epoch 200 :  0.000775808\n",
      "생성된 문장: Hello, this is a sample sentence for training RNN language model. \n",
      "loss at epoch 210 :  0.000448106\n",
      "loss at epoch 220 :  0.000259399\n",
      "loss at epoch 230 :  0.0001508\n",
      "loss at epoch 240 :  8.86916e-05\n",
      "loss at epoch 250 :  5.26905e-05\n",
      "loss at epoch 260 :  3.27825e-05\n",
      "loss at epoch 270 :  2.02656e-05\n",
      "loss at epoch 280 :  1.41859e-05\n",
      "loss at epoch 290 :  9.29832e-06\n",
      "loss at epoch 300 :  7.03335e-06\n",
      "생성된 문장: Hello, this is a sample sentence for training RNN language model. \n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    tf.initialize_all_variables().run()\n",
    "    for e in range(num_epochs):\n",
    "        _, l = sess.run([optimizer, loss])\n",
    "        if e % 10 == 0:\n",
    "            print \"loss at epoch %d\" %e,\": \", l   # 문장에 대한 softmax cross entropy loss 값\n",
    "        if e % 100 == 0:\n",
    "            idxs = sess.run(max_char)\n",
    "            sentence = [idx_to_char[idx] for idx in idxs]\n",
    "            print \"생성된 문장: %s\" %(start_char+''.join(sentence))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
